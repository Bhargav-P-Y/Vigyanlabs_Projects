{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016819b5-4a88-42c7-9481-6d14a3c81c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhargav\\anaconda3\\lib\\site-packages (from requests->transformers) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621405de-b286-46a7-aeef-9806e066f660",
   "metadata": {},
   "source": [
    "The pipeline fucntion from the transformers library, does all the preprocessing, sending & processing it to a model and post processing the result for you under the hood <br>\n",
    "We just need to specify the type of task, like text-genration, summarization etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99208d28-acc7-4789-a46f-4d45035e8c1e",
   "metadata": {},
   "source": [
    "### Zero-Shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6122ff31-9538-4c96-8738-39480eba87cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db64cd5b095b4e8eb4046e4684297b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhargav\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhargav\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea54775424b49f98f770a0baba2b32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21719b34de64200a90243a707f5a507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd15b7ed622b46e4a33da534f63e7a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03b2c431e8a48a4998b287ca9f4053b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0045ac8765b4c618846f2a3e327755a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445985913276672, 0.11197449266910553, 0.04342692717909813]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3881808-735c-48c6-97e2-438621a2c6e6",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fd56ca-770b-4b56-ace4-00bc1ebb6987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86545b4020bb416ca9d651265a1d304e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhargav\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhargav\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87aa5c923820427996271f93d7123537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184c8dadd94542aabf37ade44cb3439a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fd87685ef5479ab15f6563a3278141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\bhargav\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9499646),\n",
       "  'word': 'Bhargav',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.99566317),\n",
       "  'word': 'Vigyanlabs Innovation',\n",
       "  'start': 33,\n",
       "  'end': 54},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9888785),\n",
       "  'word': 'Mysore',\n",
       "  'start': 58,\n",
       "  'end': 64}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Bhargav and I work at Vigyanlabs Innovation in Mysore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983c3aff-5076-4268-9eb4-4fb2f745b64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd340d8bc3247f9a87c90b7ebfdd21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhargav\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhargav\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7e79c7eb384ab7ade3eecc4b0c533f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d682afc7200145a58b7c0a5ccf08f7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acacd3e7303d4d408e4ca88337a004af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b845a14f17cd4ef997b1a945b905bcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e81c9a8085948e8ab93b4fc57dfea78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64f5c20171a47f3899cc013b958624b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'AI Engineering roles are available.\\n\\n• This position is available to students, faculty, staff, and students from the Department of Aviation Engineering and Applied Sciences and the University of Maryland. (For information on current and future positions, please refer to the Aviation Engineering and Applied Sciences and Department of Aviation Engineering and Applied Sciences roles and positions information page).\\n\\n• This position is available to students, faculty, staff, and students from the Department of Aviation Engineering and Applied Sciences (UMD) and the University of Maryland. (For information on current and future positions, please refer to the Aviation Engineering and Applied Sciences and Department of Aviation Engineering and Applied Sciences roles and positions information page).\\n\\n• This position is available to students, faculty, staff, and students from the Department of Aviation Engineering and Applied Sciences (UMD) and the University of Maryland. (For information on current and future positions, please refer to the Aviation Engineering and Applied Sciences and Department of Aviation Engineering and Applied Sciences roles and positions information page).\\n\\n• This position is available to students, faculty, staff, and students from the Department of Aviation Engineering and Applied Sciences (UMD) and the University of Maryland. (For information on current and future positions, please refer to the Aviation Engineering and Applied Sciences and Department of Aviation'},\n",
       " {'generated_text': 'AI Engineering roles are available in the following positions:\\n\\n\\nWe are recruiting engineers who have been selected by the U.S. Department of Education and supported by the Federal Reserve. We are looking for experienced, experienced engineers who are seeking to work in the field of engineering. Applications will be accepted through the U.S. Department of Labor.\\n\\n\\nApplicants will be selected by the U.S. Department of Energy and the United States Department of Agriculture. Applications will be accepted through the U.S. Department of Labor.\\n\\n\\nApplications will be accepted through the U.S. Department of Commerce at U.S. Department of Commerce.\\n\\n\\nQualifications are subject to approval by the U.S. Department of Education.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(task = 'text-generation')\n",
    "generator('AI Engineering roles are',\n",
    "         max_length = 30,\n",
    "         num_return_sequences = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a3200-eec3-43df-b9dc-c76092b4a47b",
   "metadata": {},
   "source": [
    "Generally, tasks requiring <br>\n",
    "bidirectional context (words that are both behind and in front of the word to predict) use encoders, BERT<br>\n",
    "tasks generating text use decoders, GPT <br>\n",
    "and tasks converting one sequence to another use encoder-decoders. T5 & BART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e331c3b-570d-4d8f-9cd0-6d5da9cc0764",
   "metadata": {},
   "source": [
    "BART combines strenths of BERT and GPT. <br>\n",
    "It masks entire sequence of words with a single MASK token. THe model needs to predict all the missing words as well as the unmasked words <br>\n",
    "\n",
    "Imagine BART is an English-speaking teacher.<br>\n",
    "To help students who speak French, you hire a French-speaking assistant (new encoder). <br>\n",
    "\n",
    "At first, only the assistant learns how to explain French words to the teacher.<br>\n",
    "\n",
    "Later, both the assistant and teacher train together to improve communication.<br>\n",
    "\n",
    "Finally, you hire a multilingual assistant (mBART) who already knows many languages, so you don’t need separate assistants for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbcde0-7fc6-44b9-82b1-c4cf1cc0f7d0",
   "metadata": {},
   "source": [
    "Decoder models are called auto-regressive models. <br>\n",
    "THey generate output only based on the previous words it predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce16e7-5812-4e3a-bb19-a597ae52aef1",
   "metadata": {},
   "source": [
    "Pretrainig of encoder-decoder models is based first on: <br>\n",
    "Corrupting input: randomly taking spans of text and replacing it with a mask token <br>\n",
    "THen the decoder has to predict the words that were masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7e3c7-47cf-4bb4-981b-9af27655ec03",
   "metadata": {},
   "source": [
    "## Final Year Project\n",
    "\n",
    "Seq-to-Seq + RAG + LLM\n",
    "\n",
    "Speech to text -> LLM + RAG --> Text --> Text-to-Speech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
